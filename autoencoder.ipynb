{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression,stats\n",
    "import statsmodels.stats.diagnostic as smd\n",
    "import scipy\n",
    "\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.layers import Input, Dense,Conv1D, Conv2D, MaxPooling2D, UpSampling2D,dot,Reshape,add,Lambda\n",
    "from keras.layers import BatchNormalization,LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "from tensorflow import set_random_seed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the function of getting and processing raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    cha=pd.read_csv(''.join(('F://capstone//xz//0711//0724//autoencoder//',name,'.csv')))\n",
    "    cha.date=pd.to_datetime(cha.date)\n",
    "    cha.set_index('date',inplace=True,drop=True)\n",
    "    return cha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data. Each file stores its portfolio's characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are thirteen porfolios.\n",
    "Rm_cha=get_data('Rm_cha')\n",
    "SMB_cha=get_data('SMB_cha')\n",
    "HML_cha=get_data('HML_cha')\n",
    "UMD_cha=get_data('UMD_cha')\n",
    "RMW_cha=get_data('RMW_cha')\n",
    "CMA_cha=get_data('CMA_cha')\n",
    "VOL_cha=get_data('VOL_cha')\n",
    "DIV_cha=get_data('DIV_cha')\n",
    "EPS_cha=get_data('EPS_cha')\n",
    "ACC_cha=get_data('ACC_cha')\n",
    "CAS_cha=get_data('CAS_cha')\n",
    "SDE_cha=get_data('SDE_cha')\n",
    "LDE_cha=get_data('LDE_cha')\n",
    "REV_cha=get_data('REV_cha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For example, SMB_cha is the porfolio constructed by longing small market cap stocks and shorting big market cap stocks, the characteristic 'SMB' is its market capital, 'HML' is its value weighted B/M ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>UMD</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>VOL</th>\n",
       "      <th>DIV</th>\n",
       "      <th>EPS</th>\n",
       "      <th>ACC</th>\n",
       "      <th>CAS</th>\n",
       "      <th>SDE</th>\n",
       "      <th>LDE</th>\n",
       "      <th>REV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-02</th>\n",
       "      <td>-8.023295e+06</td>\n",
       "      <td>0.201507</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.018662</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>5724.964787</td>\n",
       "      <td>-0.208567</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>-4084.234516</td>\n",
       "      <td>-1574.735352</td>\n",
       "      <td>-31666.357271</td>\n",
       "      <td>-24293.832395</td>\n",
       "      <td>-10162.269872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-03</th>\n",
       "      <td>-8.098141e+06</td>\n",
       "      <td>0.200724</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>-0.018784</td>\n",
       "      <td>0.042070</td>\n",
       "      <td>5554.409064</td>\n",
       "      <td>-0.212329</td>\n",
       "      <td>0.029929</td>\n",
       "      <td>-4057.971624</td>\n",
       "      <td>-1573.467536</td>\n",
       "      <td>-31483.932426</td>\n",
       "      <td>-24111.965236</td>\n",
       "      <td>-10110.574436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-04</th>\n",
       "      <td>-8.140646e+06</td>\n",
       "      <td>0.194394</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.017310</td>\n",
       "      <td>0.041485</td>\n",
       "      <td>5484.945361</td>\n",
       "      <td>-0.213540</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>-4057.041199</td>\n",
       "      <td>-1566.668457</td>\n",
       "      <td>-31796.722488</td>\n",
       "      <td>-24112.342570</td>\n",
       "      <td>-10095.897005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-07</th>\n",
       "      <td>-8.084675e+06</td>\n",
       "      <td>0.197844</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>-0.012883</td>\n",
       "      <td>0.041576</td>\n",
       "      <td>5121.380471</td>\n",
       "      <td>-0.095240</td>\n",
       "      <td>0.029585</td>\n",
       "      <td>-4037.724371</td>\n",
       "      <td>-1516.500206</td>\n",
       "      <td>-31377.816696</td>\n",
       "      <td>-23666.795541</td>\n",
       "      <td>-10061.890052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-08</th>\n",
       "      <td>-8.056239e+06</td>\n",
       "      <td>0.200182</td>\n",
       "      <td>-0.000851</td>\n",
       "      <td>-0.013327</td>\n",
       "      <td>0.041935</td>\n",
       "      <td>5250.372168</td>\n",
       "      <td>-0.090106</td>\n",
       "      <td>0.029133</td>\n",
       "      <td>-4048.863424</td>\n",
       "      <td>-1519.905823</td>\n",
       "      <td>-30957.048602</td>\n",
       "      <td>-23482.164649</td>\n",
       "      <td>-10064.931672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     SMB       HML       UMD       RMW       CMA          VOL  \\\n",
       "date                                                                            \n",
       "2002-01-02 -8.023295e+06  0.201507 -0.007754 -0.018662  0.042474  5724.964787   \n",
       "2002-01-03 -8.098141e+06  0.200724 -0.002183 -0.018784  0.042070  5554.409064   \n",
       "2002-01-04 -8.140646e+06  0.194394 -0.002329 -0.017310  0.041485  5484.945361   \n",
       "2002-01-07 -8.084675e+06  0.197844 -0.000882 -0.012883  0.041576  5121.380471   \n",
       "2002-01-08 -8.056239e+06  0.200182 -0.000851 -0.013327  0.041935  5250.372168   \n",
       "\n",
       "                 DIV       EPS          ACC          CAS           SDE  \\\n",
       "date                                                                     \n",
       "2002-01-02 -0.208567  0.029950 -4084.234516 -1574.735352 -31666.357271   \n",
       "2002-01-03 -0.212329  0.029929 -4057.971624 -1573.467536 -31483.932426   \n",
       "2002-01-04 -0.213540  0.029525 -4057.041199 -1566.668457 -31796.722488   \n",
       "2002-01-07 -0.095240  0.029585 -4037.724371 -1516.500206 -31377.816696   \n",
       "2002-01-08 -0.090106  0.029133 -4048.863424 -1519.905823 -30957.048602   \n",
       "\n",
       "                     LDE           REV  \n",
       "date                                    \n",
       "2002-01-02 -24293.832395 -10162.269872  \n",
       "2002-01-03 -24111.965236 -10110.574436  \n",
       "2002-01-04 -24112.342570 -10095.897005  \n",
       "2002-01-07 -23666.795541 -10061.890052  \n",
       "2002-01-08 -23482.164649 -10064.931672  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMB_cha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_factor=pd.read_csv('F://capstone//xz//0711//0724//autoencoder//total_factor.csv')\n",
    "total_factor.date=pd.to_datetime(total_factor.date)\n",
    "total_factor.set_index('date',inplace=True,drop=True)\n",
    "total_factor.columns=['SMB_port','HML_port','UMD_port','RMW_port','CMA_port','VOL_port',\n",
    "                            'DIV_port','EPS_port','ACC_port','CAS_port','SDE_port','LDE_port','REV_port']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is time series returns of different portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMB_port</th>\n",
       "      <th>HML_port</th>\n",
       "      <th>UMD_port</th>\n",
       "      <th>RMW_port</th>\n",
       "      <th>CMA_port</th>\n",
       "      <th>VOL_port</th>\n",
       "      <th>DIV_port</th>\n",
       "      <th>EPS_port</th>\n",
       "      <th>ACC_port</th>\n",
       "      <th>CAS_port</th>\n",
       "      <th>SDE_port</th>\n",
       "      <th>LDE_port</th>\n",
       "      <th>REV_port</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-02</th>\n",
       "      <td>0.003815</td>\n",
       "      <td>-0.005121</td>\n",
       "      <td>-0.014339</td>\n",
       "      <td>-0.014818</td>\n",
       "      <td>-0.009994</td>\n",
       "      <td>0.017023</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>-0.009336</td>\n",
       "      <td>-0.000522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-03</th>\n",
       "      <td>-0.005394</td>\n",
       "      <td>-0.006833</td>\n",
       "      <td>-0.019648</td>\n",
       "      <td>-0.032788</td>\n",
       "      <td>-0.012066</td>\n",
       "      <td>0.036267</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>-0.014535</td>\n",
       "      <td>-0.003106</td>\n",
       "      <td>-0.026694</td>\n",
       "      <td>-0.029026</td>\n",
       "      <td>-0.010790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-04</th>\n",
       "      <td>-0.008338</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.002314</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.006936</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.008373</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>-0.010519</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>-0.002679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SMB_port  HML_port  UMD_port  RMW_port  CMA_port  VOL_port  \\\n",
       "date                                                                     \n",
       "2002-01-02  0.003815 -0.005121 -0.014339 -0.014818 -0.009994  0.017023   \n",
       "2002-01-03 -0.005394 -0.006833 -0.019648 -0.032788 -0.012066  0.036267   \n",
       "2002-01-04 -0.008338  0.006134 -0.011261 -0.002314 -0.006851  0.006936   \n",
       "\n",
       "            DIV_port  EPS_port  ACC_port  CAS_port  SDE_port  LDE_port  \\\n",
       "date                                                                     \n",
       "2002-01-02  0.004469  0.018734  0.000855  0.006003 -0.005749 -0.009336   \n",
       "2002-01-03 -0.001241  0.007677 -0.014535 -0.003106 -0.026694 -0.029026   \n",
       "2002-01-04  0.000790  0.008373 -0.004631 -0.010519 -0.000818  0.002526   \n",
       "\n",
       "            REV_port  \n",
       "date                  \n",
       "2002-01-02 -0.000522  \n",
       "2002-01-03 -0.010790  \n",
       "2002-01-04 -0.002679  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_factor.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get date series of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date=total_factor.loc[pd.Timestamp(2002,1,1):pd.Timestamp(2014,12,31),:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2002-01-02', '2002-01-03', '2002-01-04', '2002-01-07',\n",
       "               '2002-01-08', '2002-01-09', '2002-01-10', '2002-01-11',\n",
       "               '2002-01-14', '2002-01-15',\n",
       "               ...\n",
       "               '2014-12-17', '2014-12-18', '2014-12-19', '2014-12-22',\n",
       "               '2014-12-23', '2014-12-24', '2014-12-26', '2014-12-29',\n",
       "               '2014-12-30', '2014-12-31'],\n",
       "              dtype='datetime64[ns]', name='date', length=3273, freq=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get date series of training data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_date=total_factor.loc[pd.Timestamp(2015,1,1):end_date[-1],:].index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I used dictionary to store porfolios' characteristics. The key is date. I also rescaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4275,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_factor.index[4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4275,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_factor.index[:-4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>UMD</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>VOL</th>\n",
       "      <th>DIV</th>\n",
       "      <th>EPS</th>\n",
       "      <th>ACC</th>\n",
       "      <th>CAS</th>\n",
       "      <th>SDE</th>\n",
       "      <th>LDE</th>\n",
       "      <th>REV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-01-02</th>\n",
       "      <td>-8.023295e+06</td>\n",
       "      <td>0.201507</td>\n",
       "      <td>-0.007754</td>\n",
       "      <td>-0.018662</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>5724.964787</td>\n",
       "      <td>-0.208567</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>-4084.234516</td>\n",
       "      <td>-1574.735352</td>\n",
       "      <td>-31666.357271</td>\n",
       "      <td>-24293.832395</td>\n",
       "      <td>-10162.269872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-03</th>\n",
       "      <td>-8.098141e+06</td>\n",
       "      <td>0.200724</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>-0.018784</td>\n",
       "      <td>0.042070</td>\n",
       "      <td>5554.409064</td>\n",
       "      <td>-0.212329</td>\n",
       "      <td>0.029929</td>\n",
       "      <td>-4057.971624</td>\n",
       "      <td>-1573.467536</td>\n",
       "      <td>-31483.932426</td>\n",
       "      <td>-24111.965236</td>\n",
       "      <td>-10110.574436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-01-04</th>\n",
       "      <td>-8.140646e+06</td>\n",
       "      <td>0.194394</td>\n",
       "      <td>-0.002329</td>\n",
       "      <td>-0.017310</td>\n",
       "      <td>0.041485</td>\n",
       "      <td>5484.945361</td>\n",
       "      <td>-0.213540</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>-4057.041199</td>\n",
       "      <td>-1566.668457</td>\n",
       "      <td>-31796.722488</td>\n",
       "      <td>-24112.342570</td>\n",
       "      <td>-10095.897005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     SMB       HML       UMD       RMW       CMA          VOL  \\\n",
       "date                                                                            \n",
       "2002-01-02 -8.023295e+06  0.201507 -0.007754 -0.018662  0.042474  5724.964787   \n",
       "2002-01-03 -8.098141e+06  0.200724 -0.002183 -0.018784  0.042070  5554.409064   \n",
       "2002-01-04 -8.140646e+06  0.194394 -0.002329 -0.017310  0.041485  5484.945361   \n",
       "\n",
       "                 DIV       EPS          ACC          CAS           SDE  \\\n",
       "date                                                                     \n",
       "2002-01-02 -0.208567  0.029950 -4084.234516 -1574.735352 -31666.357271   \n",
       "2002-01-03 -0.212329  0.029929 -4057.971624 -1573.467536 -31483.932426   \n",
       "2002-01-04 -0.213540  0.029525 -4057.041199 -1566.668457 -31796.722488   \n",
       "\n",
       "                     LDE           REV  \n",
       "date                                    \n",
       "2002-01-02 -24293.832395 -10162.269872  \n",
       "2002-01-03 -24111.965236 -10110.574436  \n",
       "2002-01-04 -24112.342570 -10095.897005  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMB_cha.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I get beg_date and end_date for every five trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_date=[]\n",
    "end_date=[]\n",
    "for i in range(0,len(total_factor.index)-4,5):\n",
    "    beg_date.append(total_factor.index[i])\n",
    "    end_date.append(total_factor.index[i+4])\n",
    "#beg_date.append(total_factor.index[i+5])\n",
    "#end_date.append(total_factor.index[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I used dict to store portfolios trading data for each five trading days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_dict={}\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))  \n",
    "\n",
    "for beg,end in zip(beg_date,end_date):\n",
    "    factor_dict[beg]=np.vstack((SMB_cha.loc[beg:end,:].values.flatten(),\n",
    "                                HML_cha.loc[beg:end,:].values.flatten(),\n",
    "                                UMD_cha.loc[beg:end,:].values.flatten(),\n",
    "                                RMW_cha.loc[beg:end,:].values.flatten(),\n",
    "                                CMA_cha.loc[beg:end,:].values.flatten(),\n",
    "                                VOL_cha.loc[beg:end,:].values.flatten(),\n",
    "                                DIV_cha.loc[beg:end,:].values.flatten(),\n",
    "                                EPS_cha.loc[beg:end,:].values.flatten(),\n",
    "                                ACC_cha.loc[beg:end,:].values.flatten(),\n",
    "                                CAS_cha.loc[beg:end,:].values.flatten(),\n",
    "                                SDE_cha.loc[beg:end,:].values.flatten(), \n",
    "                                LDE_cha.loc[beg:end,:].values.flatten(),\n",
    "                                REV_cha.loc[beg:end,:].values.flatten()))\n",
    "    factor_dict[beg]=pd.DataFrame(factor_dict[beg])\n",
    "    factor_dict[beg].index=['SMB_port','HML_port','UMD_port','RMW_port','CMA_port','VOL_port',\n",
    "                            'DIV_port','EPS_port','ACC_port','CAS_port','SDE_port','LDE_port','REV_port']\n",
    "    \n",
    "    curr_date_list=SMB_cha.loc[beg:end,:].index\n",
    "    \n",
    "    for date in curr_date_list[1:]:\n",
    "        factor_dict[date]=factor_dict[beg]\n",
    "    \n",
    "    for date in curr_date_list:\n",
    "        for name in factor_dict[date].columns:\n",
    "            factor_dict[date].loc[:,[name]]=min_max_scaler.fit_transform(factor_dict[date].loc[:,[name]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get beta and factor arrays to train\n",
    "beta_train=np.array([factor_dict[train_date[0]].values])\n",
    "for date in train_date[1:]:\n",
    "    beta_train=np.append(beta_train,np.array([factor_dict[date].values]),axis=0)\n",
    "        \n",
    "fac_train=np.array([total_factor.loc[[train_date[0]],'SMB_port':].values.reshape(13)])\n",
    "for date in train_date[1:]:\n",
    "    fac_train=np.append(fac_train,np.array([total_factor.loc[[date],'SMB_port':].values.reshape(13)]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_train_shape:  (3273, 13, 65)\n",
      "fac_train_shape:  (3273, 13)\n"
     ]
    }
   ],
   "source": [
    "print('beta_train_shape: ',beta_train.shape)\n",
    "print('fac_train_shape: ',fac_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below I built autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 13, 65)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4)            56          input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 13, 4)        264         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 13)           0           dense_10[0][0]                   \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 13)           0           dot_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 4  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac,kernel_regularizer=regularizers.l2(0.001))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "#='tanh')(input_img_beta)\n",
    "\n",
    "#encoded_beta= Dense(encoding_dim_fac)(input_img_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_1 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_1.compile(optimizer='Adam', loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "autoencoder_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 13, 65)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 13, 13)       858         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            56          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 13, 4)        56          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 13)           0           dense_3[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 13)           0           dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 970\n",
      "Trainable params: 970\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 4  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13)(input_img_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_2 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_2.compile(optimizer='Adam', loss='mean_squared_error',metrics=['mse'])\n",
    "autoencoder_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 13, 65)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 13, 13)       858         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 13, 13)       182         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            56          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 13, 4)        56          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 13)           0           dense_6[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 13)           0           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,152\n",
      "Trainable params: 1,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 4  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13)(input_img_beta)\n",
    "encoded_beta= Dense(13)(encoded_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_3 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_3.compile(optimizer='Adam', loss='mean_squared_error',metrics=['mse'])\n",
    "autoencoder_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_pred_stat(model): \n",
    "    #fid model\n",
    "    seed(1) \n",
    "    set_random_seed(2)\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
    "                 #ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "    \n",
    "    history=model.fit([fac_train,beta_train], fac_train,batch_size=500, epochs=1000,verbose=0)#,\n",
    "                      #validation_split=0.2,\n",
    "                      #callbacks=callbacks)\n",
    "    \n",
    "    # get predict returns\n",
    "    return_predict=model.predict([fac_train,beta_train])\n",
    "    return_predict_pd=pd.DataFrame(return_predict,columns=['SMB_port','HML_port','UMD_port','RMW_port',\n",
    "                                                       'CMA_port','VOL_port','DIV_port','EPS_port',\n",
    "                                                       'ACC_port','CAS_port','SDE_port','LDE_port',\n",
    "                                                       'REV_port'], index=train_date)\n",
    "    # calculate correlation\n",
    "    corr=[]\n",
    "    for name in return_predict_pd.columns:\n",
    "        corr.append(pd.concat((return_predict_pd[name],total_factor[name]),axis=1).corr().iloc[0,1])\n",
    "    corr=pd.DataFrame(corr,index=['SMB_port','HML_port','UMD_port','RMW_port',\n",
    "                                  'CMA_port','VOL_port','DIV_port','EPS_port',\n",
    "                                  'ACC_port','CAS_port','SDE_port','LDE_port',\n",
    "                                  'REV_port'],columns=['auto_corr'])\n",
    "    \n",
    "    # calculate r_square \n",
    "    r_square=[]\n",
    "    for name in return_predict_pd.columns:\n",
    "        diff_square_sum=((return_predict_pd.loc[train_date,name]-total_factor.loc[train_date,name])**2).sum()\n",
    "        square_sum=((total_factor.loc[train_date,name])**2).sum()\n",
    "        r_square.append(1-diff_square_sum/square_sum)\n",
    "        #r_square.append(diff_square_sum)\n",
    "    r_square=pd.DataFrame(r_square,index=['SMB_port','HML_port','UMD_port','RMW_port',\n",
    "                                                           'CMA_port','VOL_port','DIV_port','EPS_port',\n",
    "                                                           'ACC_port','CAS_port','SDE_port','LDE_port',\n",
    "                                                           'REV_port'],columns=['auto_r_square'])\n",
    "    mse=((return_predict-fac_train)**2/(fac_train.shape[0]*fac_train.shape[1])).sum()\n",
    "    return (return_predict_pd,corr,r_square,history,mse)#,return_predict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_return_pred_1,auto_corr_1,auto_r_square_1,history_1,mse_1=auto_pred_stat(autoencoder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27d431c5dd8>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27d43250780>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type \"<class 'list'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-328a02a615af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    284\u001b[0m                        \u001b[1;34m' only pd.Series, pd.DataFrame, and pd.Panel'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                        ' (deprecated) objs are valid'.format(type(obj)))\n\u001b[1;32m--> 286\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type \"<class 'list'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid"
     ]
    }
   ],
   "source": [
    "pd.concat((history_1.history['loss'],history_1.history['mean_squared_error']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "auto_return_pred_1,auto_corr_1,auto_r_square_1,history_1,mse_1=auto_pred_stat(autoencoder_1)\n",
    "\n",
    "auto_return_pred_2,auto_corr_2,auto_r_square_2,history_2,mse_2=auto_pred_stat(autoencoder_2)\n",
    "\n",
    "auto_return_pred_3,auto_corr_3,auto_r_square_3,history_3,mse_3=auto_pred_stat(autoencoder_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F_F_r_square' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-5534d46e88e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauto_four_r_square\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauto_r_square_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_r_square_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_r_square_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF_F_r_square\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mauto_four_r_square\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'auto_r_square_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'auto_r_square_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'auto_r_square_3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F_F_r_square'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mauto_four_r_square\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F_F_r_square' is not defined"
     ]
    }
   ],
   "source": [
    "auto_four_r_square=pd.concat((auto_r_square_1,auto_r_square_2,auto_r_square_3,F_F_r_square),axis=1)\n",
    "auto_four_r_square.columns=['auto_r_square_1','auto_r_square_2','auto_r_square_3','F_F_r_square']\n",
    "auto_four_r_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_four_loss=pd.DataFrame([history_1.history['loss'],history_2.history['loss'],history_3.history['loss']],\n",
    "                           index=['history_1_loss','history_2_loss','history_3_loss']).T\n",
    "auto_four_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_four_r_square_mse=pd.DataFrame([[mse_1,mse_2,mse_3]],index=['mse'],columns=['mse_1','mse_2','mse_3'])\n",
    "auto_four_r_square_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_four_mse=pd.DataFrame([history_1.history['mean_squared_error'],history_2.history['mean_squared_error'],\n",
    "                            history_3.history['mean_squared_error']],\n",
    "                           index=['history_1_mse','history_2_mse','history_3_mse']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_four_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1, 5 factor\n",
    "## this model has 1 dense in beta side layer and 5 factor in bottleneck layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 5  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_1_5 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_1_5.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1, 6 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 6  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_1_6 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_1_6.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1, 7 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 7  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_1_7 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_1_7.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1, 8 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 8  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_1_8 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_1_8.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_return_pred_1_5,auto_corr_1_5,auto_r_square_1_5,history_1_5,mse_1_5=auto_pred_stat(autoencoder_1_5)\n",
    "\n",
    "auto_return_pred_1_6,auto_corr_1_6,auto_r_square_1_6,history_1_6,mse_1_6=auto_pred_stat(autoencoder_1_6)\n",
    "\n",
    "auto_return_pred_1_7,auto_corr_1_7,auto_r_square_1_7,history_1_7,mse_1_7=auto_pred_stat(autoencoder_1_7)\n",
    "\n",
    "auto_return_pred_1_8,auto_corr_1_8,auto_r_square_1_8,history_1_8,mse_1_8=auto_pred_stat(autoencoder_1_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_r_square=pd.concat((auto_r_square_1,auto_r_square_1_5,auto_r_square_1_6,\n",
    "                                 auto_r_square_1_7,auto_r_square_1_8),axis=1)\n",
    "auto_one_four_r_square.columns=['auto_r_square_1_4','auto_r_square_1_5','auto_r_square_1_6','auto_r_square_1_7',\n",
    "                                'auto_r_square_1_8']\n",
    "auto_one_four_r_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_r_square.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_one_four_r_square.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_loss=pd.DataFrame([history_1.history['loss'],history_1_5.history['loss'],history_1_6.history['loss'],\n",
    "                            history_1_7.history['loss'],history_1_8.history['loss'],],\n",
    "                           index=['history_1_4_loss',\n",
    "                                 'history_1_5_loss','history_1_6_loss','history_1_7_loss',\n",
    "                                 'history_1_8_loss']).T\n",
    "auto_one_four_loss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_loss.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_one_four_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_mse=pd.DataFrame([[mse_1,mse_1_5,mse_1_6,mse_1_7,mse_1_8]],columns=['mse_1_4','mse_1_5',\n",
    "                                                                                  'mse_1_6','mse_1_7','mse_1_8'],\n",
    "                              index=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_one_four_mse.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_one_four_mse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2, 5 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 5  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_2_5 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_2_5.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2, 6 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 6  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_2_6 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_2_6.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2, 7 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 7  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_2_7 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_2_7.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2, 8 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 8  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_2_8 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_2_8.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_return_pred_2_5,auto_corr_2_5,auto_r_square_2_5,history_2_5,mse_2_5=auto_pred_stat(autoencoder_2_5)\n",
    "\n",
    "auto_return_pred_2_6,auto_corr_2_6,auto_r_square_2_6,history_2_6,mse_2_6=auto_pred_stat(autoencoder_2_6)\n",
    "\n",
    "auto_return_pred_2_7,auto_corr_2_7,auto_r_square_2_7,history_2_7,mse_2_7=auto_pred_stat(autoencoder_2_7)\n",
    "\n",
    "auto_return_pred_2_8,auto_corr_2_8,auto_r_square_2_8,history_2_8,mse_2_8=auto_pred_stat(autoencoder_2_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_r_square=pd.concat((auto_r_square_2,auto_r_square_2_5,auto_r_square_2_6,\n",
    "                                 auto_r_square_2_7,auto_r_square_2_8),axis=1)\n",
    "auto_two_four_r_square.columns=['auto_r_square_2_4','auto_r_square_2_5','auto_r_square_2_6','auto_r_square_2_7',\n",
    "                                'auto_r_square_2_8']\n",
    "auto_two_four_r_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_r_square.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_two_four_r_square.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_loss=pd.DataFrame([history_2.history['loss'],history_2_5.history['loss'],history_2_6.history['loss'],\n",
    "                            history_2_7.history['loss'],history_2_8.history['loss'],],\n",
    "                           index=['history_2_4_loss',\n",
    "                                 'history_2_5_loss','history_2_6_loss','history_2_7_loss',\n",
    "                                 'history_2_8_loss']).T\n",
    "auto_two_four_loss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_loss.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_two_four_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_mse=pd.DataFrame([[mse_2,mse_2_5,mse_2_6,mse_2_7,mse_2_8]],columns=['mse_2_4','mse_2_5',\n",
    "                                                                                  'mse_2_6','mse_2_7','mse_2_8'],\n",
    "                              index=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_two_four_mse.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_two_four_mse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3, 5 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 5  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_3_5 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_3_5.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3, 6 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 6  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_3_6 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_3_6.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3, 7 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 7  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_3_7 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_3_7.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3, 8 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1) \n",
    "set_random_seed(2)\n",
    "# I use four factors to do predition\n",
    "encoding_dim_fac = 8  \n",
    "\n",
    "# this is our factor encoded representations\n",
    "# the input shape is (13,), meaning the returns of 13 portfolios \n",
    "input_img_fac = Input(shape=(13,))\n",
    "encoded_fac = Dense(encoding_dim_fac)(input_img_fac)#,kernel_regularizer=regularizers.l2(0.01))(input_img_fac)\n",
    "encoded_fac_model=Model(input_img_fac, encoded_fac)\n",
    "\n",
    "\n",
    "# this is our beta encoded representations\n",
    "# the input shape is (13,65), meaning that 13 portfolios have 65 characteristics\n",
    "input_img_beta = Input(shape=(13,65))\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(input_img_beta)\n",
    "encoded_beta= Dense(13,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta= Dense(encoding_dim_fac,activation=LeakyReLU(alpha=0.01))(encoded_beta)\n",
    "encoded_beta_model=Model(input_img_beta, encoded_beta)\n",
    "\n",
    "# combine together\n",
    "# dot the beta layers and factor layers\n",
    "encoded_com= dot([encoded_fac,encoded_beta],axes=[1,2])\n",
    "encoded_com_layer=Lambda(lambda x: x)(encoded_com)\n",
    "\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "#decoded = Dense(13)(encoded_com_layer)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder_3_8 = Model(inputs=[input_img_fac,input_img_beta], outputs=encoded_com_layer)\n",
    "\n",
    "autoencoder_3_8.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_return_pred_3_5,auto_corr_3_5,auto_r_square_3_5,history_3_5,mse_3_5=auto_pred_stat(autoencoder_3_5)\n",
    "\n",
    "auto_return_pred_3_6,auto_corr_3_6,auto_r_square_3_6,history_3_6,mse_3_6=auto_pred_stat(autoencoder_3_6)\n",
    "\n",
    "auto_return_pred_3_7,auto_corr_3_7,auto_r_square_3_7,history_3_7,mse_3_7=auto_pred_stat(autoencoder_3_7)\n",
    "\n",
    "auto_return_pred_3_8,auto_corr_3_8,auto_r_square_3_8,history_3_8,mse_3_8=auto_pred_stat(autoencoder_3_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_r_square=pd.concat((auto_r_square_3,auto_r_square_3_5,auto_r_square_3_6,\n",
    "                                 auto_r_square_3_7,auto_r_square_3_8),axis=1)\n",
    "auto_three_four_r_square.columns=['auto_r_square_3_4','auto_r_square_3_5','auto_r_square_3_6','auto_r_square_3_7',\n",
    "                                'auto_r_square_3_8']\n",
    "auto_three_four_r_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_r_square.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_three_four_r_square.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_loss=pd.DataFrame([history_3.history['loss'],history_3_5.history['loss'],history_3_6.history['loss'],\n",
    "                            history_3_7.history['loss'],history_3_8.history['loss'],],\n",
    "                           index=['history_3_4_loss',\n",
    "                                 'history_3_5_loss','history_3_6_loss','history_3_7_loss',\n",
    "                                 'history_3_8_loss']).T\n",
    "auto_three_four_loss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_loss.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_three_four_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_mse=pd.DataFrame([[mse_3,mse_3_5,mse_3_6,mse_3_7,mse_3_8]],columns=['mse_3_5','mse_3_5',\n",
    "                                                                                  'mse_3_6','mse_3_7','mse_3_8'],\n",
    "                              index=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_mse.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\auto_three_four_mse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_three_four_loss.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get F_F factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get factor data\n",
    "\n",
    "#### This is Famm and french factors, SMB has eliminated the effect of Rm_Rf, other factors except Rm_Rf have eliminated the effects of Rm_Rf and SMB.\n",
    "\n",
    "Rm_Rf=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\Rm_Rf_F_F_decorr.csv')\n",
    "Rm_Rf.date=pd.to_datetime(Rm_Rf.date)\n",
    "Rm_Rf.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "SMB=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\SMB_F_F_decorr.csv')\n",
    "SMB.date=pd.to_datetime(SMB.date)\n",
    "SMB.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "HML=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\HML_F_F_decorr.csv')\n",
    "HML.date=pd.to_datetime(HML.date)\n",
    "HML.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "UMD=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\UMD_F_F_decorr.csv')\n",
    "UMD.date=pd.to_datetime(UMD.date)\n",
    "UMD.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "RMW=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\RMW_F_F_decorr.csv')\n",
    "RMW.date=pd.to_datetime(RMW.date)\n",
    "RMW.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "CMA=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\CMA_F_F_decorr.csv')\n",
    "CMA.date=pd.to_datetime(CMA.date)\n",
    "CMA.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "VOL=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\VOL_F_F_decorr.csv')\n",
    "VOL.date=pd.to_datetime(VOL.date)\n",
    "VOL.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "# HML_RMW_CMA_UMD is the average of HML,RMW,CMA,UMD\n",
    "HML_RMW_CMA_UMD=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\HML_RMW_CMA_UMD_F_F_decorr.csv')\n",
    "HML_RMW_CMA_UMD.date=pd.to_datetime(HML_RMW_CMA_UMD.date)\n",
    "HML_RMW_CMA_UMD.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "Rf=pd.read_csv(r'F:\\capstone\\xz\\0711\\0724\\Rf_F_F_decorr.csv')\n",
    "Rf.date=pd.to_datetime(Rf.date)\n",
    "Rf.set_index('date',drop=True,inplace=True)\n",
    "\n",
    "total_factor_F=pd.concat((Rm_Rf,SMB,HML,UMD,RMW,CMA,VOL,HML_RMW_CMA_UMD,Rf),axis=1)\n",
    "\n",
    "total_factor_F.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do regression by F_F factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_mom=[3,6,9,12]*17\n",
    "end_day=[31,30,30,31]*17\n",
    "end_year=[2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014]*4\n",
    "end_year.sort()\n",
    "end_date_pre=[]\n",
    "for year,mom,day in zip(end_year,end_mom,end_day):\n",
    "    end_date_pre.append(pd.Timestamp(year,mom,day))\n",
    "end_date_pre=end_date_pre[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_date_pre=[(date-pd.Timedelta(days=75)).replace(day=1) for date in end_date_pre ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_date=[]\n",
    "end_date=[]\n",
    "for beg,end in zip(beg_date_pre,end_date_pre):\n",
    "    beg_1=total_factor_F.loc[beg:end,:].index[0]\n",
    "    end_1=total_factor_F.loc[beg:end,:].index[-1]\n",
    "    beg_date.append(beg_1)\n",
    "    end_date.append(end_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_F_pred_total_pd=pd.DataFrame([])\n",
    "for beg,end in zip(beg_date,end_date):\n",
    "    \n",
    "    X=total_factor_F.loc[beg:end,['Rm_Rf','SMB','HML','UMD']]\n",
    "    X=sm.add_constant(X)\n",
    "    F_F_pred_pd=pd.DataFrame([])\n",
    "    for name in total_factor.columns:\n",
    "        Y=total_factor.loc[beg:end,name].sub(total_factor_F.loc[beg:end,'Rf'].values,axis=0)\n",
    "        model=regression.linear_model.OLS(Y,X).fit()\n",
    "        Y_pred=model.predict()\n",
    "        F_F_pred_pd=pd.concat((F_F_pred_pd,pd.DataFrame(Y_pred)),axis=1)\n",
    "    F_F_pred_pd.columns=['SMB_port','HML_port','UMD_port','RMW_port','CMA_port','VOL_port',\n",
    "                            'DIV_port','EPS_port','ACC_port','CAS_port','SDE_port','LDE_port','REV_port']\n",
    "    F_F_pred_total_pd=pd.concat((F_F_pred_total_pd,F_F_pred_pd),axis=0)\n",
    "F_F_pred_total_pd.index=train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i=0\n",
    "#j=500\n",
    "F_F_corr=[]\n",
    "for name in F_F_pred_total_pd.columns:\n",
    "    F_F_corr.append(pd.concat((F_F_pred_total_pd[name],total_factor[name]),axis=1).corr().iloc[0,1])\n",
    "F_F_corr=pd.DataFrame(F_F_corr,index=['SMB_port','HML_port','UMD_port','RMW_port',\n",
    "                                                       'CMA_port','VOL_port','DIV_port','EPS_port',\n",
    "                                                       'ACC_port','CAS_port','SDE_port','LDE_port',\n",
    "                                                       'REV_port'],columns=['F_F_corr'])\n",
    "\n",
    "F_F_r_square=[]\n",
    "for name in F_F_pred_total_pd.columns:\n",
    "    diff_square_sum=((F_F_pred_total_pd.loc[train_date,name]-total_factor.loc[train_date,name])**2).sum()\n",
    "    square_sum=((total_factor.loc[train_date,name])**2).sum()\n",
    "    F_F_r_square.append(1-diff_square_sum/square_sum)\n",
    "F_F_r_square=pd.DataFrame(F_F_r_square,index=['SMB_port','HML_port','UMD_port','RMW_port',\n",
    "                                                           'CMA_port','VOL_port','DIV_port','EPS_port',\n",
    "                                                           'ACC_port','CAS_port','SDE_port','LDE_port',\n",
    "                                                           'REV_port'],columns=['F_F_r_square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_F_r_square.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\F_F_r_square.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_F_F=((F_F_pred_total_pd-total_factor)**2/(F_F_pred_total_pd.shape[0]*F_F_pred_total_pd.shape[1])).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_F_F_=pd.DataFrame([[mse_F_F]],columns=['mse_F_F'],index=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_F_F_.to_csv(r'F:\\capstone\\xz\\0711\\0724\\autoencoder\\outcome\\mse_F_F.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
